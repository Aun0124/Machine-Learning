{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semantic analysis",
      "provenance": [],
      "authorship_tag": "ABX9TyNpOi07Jcm8ZBy3DEgsw39g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TANKIANAUN/Machine-Learning/blob/master/semantic_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUK8TUwDdMWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfpkxYKvdzAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dam8fy2_d0ch",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "4a4ec157-7bcc-4f53-ef6f-3ea80ed8bd5c"
      },
      "source": [
        "\n",
        "# Download data from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
        "\n",
        "!wget --quiet \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\"\n",
        "!unzip -q \"sentiment labelled sentences\"\n",
        "!mv \"sentiment labelled sentences\" data\n",
        "!ls -l data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 208\n",
            "-rw-r--r-- 1 root root 58226 Jul  5  2016 amazon_cells_labelled.txt\n",
            "-rw-r--r-- 1 root root 85285 Feb 15  2015 imdb_labelled.txt\n",
            "-rw-r--r-- 1 root root  1070 May 31  2015 readme.txt\n",
            "-rw-r--r-- 1 root root 61320 Jul  5  2016 yelp_labelled.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ-g59BNeDTM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "cb051e42-da8d-44d0-d9ce-5b997a43878f"
      },
      "source": [
        "!cat data/readme.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015\n",
            "Please cite the paper if you want to use it :)\n",
            "\n",
            "It contains sentences labelled with positive or negative sentiment, extracted from reviews of products, movies, and restaurants\n",
            "\n",
            "=======\n",
            "Format:\n",
            "=======\n",
            "sentence \\t score \\n\n",
            "\n",
            "\n",
            "=======\n",
            "Details:\n",
            "=======\n",
            "Score is either 1 (for positive) or 0 (for negative)\t\n",
            "The sentences come from three different websites/fields:\n",
            "\n",
            "imdb.com\n",
            "amazon.com\n",
            "yelp.com\n",
            "\n",
            "For each website, there exist 500 positive and 500 negative sentences. Those were selected randomly for larger datasets of reviews. \n",
            "We attempted to select sentences that have a clearly positive or negative connotaton, the goal was for no neutral sentences to be selected.\n",
            "\n",
            "\n",
            "\n",
            "For the full datasets look:\n",
            "\n",
            "imdb: Maas et. al., 2011 'Learning word vectors for sentiment analysis'\n",
            "amazon: McAuley et. al., 2013 'Hidden factors and hidden topics: Understanding rating dimensions with review text'\n",
            "yelp: Yelp dataset challenge http://www.yelp.com/dataset_challenge\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyy_7PvieQT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "f8dcae9b-968e-46ab-b5c3-6f2219a3ed90"
      },
      "source": [
        "\n",
        "files = ['data/yelp_labelled.txt', 'data/amazon_cells_labelled.txt', 'data/imdb_labelled.txt']\n",
        "\n",
        "df_list = []\n",
        "for file in files:\n",
        "    df = pd.read_csv(file, names=['comment', 'sentiment'], sep='\\t')\n",
        "    df_list.append(df)\n",
        "\n",
        "trainingData = pd.concat(df_list)\n",
        "\n",
        "print('Number of rows: %d' % len(trainingData))\n",
        "\n",
        "trainingData.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows: 2748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Now I am getting angry and I want my damn pho.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The potatoes were like rubber and you could te...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>The fries were great too.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>A great touch.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  sentiment\n",
              "0                           Wow... Loved this place.          1\n",
              "1                                 Crust is not good.          0\n",
              "2          Not tasty and the texture was just nasty.          0\n",
              "3  Stopped by during the late May bank holiday of...          1\n",
              "4  The selection on the menu was great and so wer...          1\n",
              "5     Now I am getting angry and I want my damn pho.          0\n",
              "6              Honeslty it didn't taste THAT fresh.)          0\n",
              "7  The potatoes were like rubber and you could te...          0\n",
              "8                          The fries were great too.          1\n",
              "9                                     A great touch.          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ1zMMPwecNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b7fa4a08-a743-47b0-c47f-42d5de6b504e"
      },
      "source": [
        "a=([1,2,3,4])\n",
        "ad=np.asarray(a)\n",
        "print(ad)\n",
        "print(a)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4]\n",
            "[1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C4twZbllJ6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "def load_glove_vectors():\n",
        "    print('Loading glove vectors...')\n",
        "    glove_map = {}\n",
        "    with open('glove.6B.%dd.txt' % EMBEDDING_DIM, encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            glove_map[word] = np.asarray(values[1:], dtype='float32')\n",
        "    return glove_map\n",
        "\n",
        "def create_embedding_matrix(word_index, num_words):\n",
        "    glove_map = load_glove_vectors()\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        if i > num_words:\n",
        "            continue\n",
        "        vector = glove_map.get(word)\n",
        "        if vector is not None:\n",
        "            embedding_matrix[i] = vector\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1elWz5jp7Ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments = trainingData.comment.astype(str).tolist()\n",
        "sentiments = trainingData.sentiment.tolist()\n",
        "labels = np.asarray(sentiments)   #the list is store in array numpy [1,2,3] to [1 2 3]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)  #set limited words\n",
        "tokenizer.fit_on_texts(comments)   #assign index lower index ,higher freq\n",
        "sequences = tokenizer.texts_to_sequences(comments)  #arrange in sequence order [1,2,3,4,5,6,7]\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "#padding [[1],[2,3],[4,5,6]] to [[0,0,1..... till maxlen]\n",
        "#                                 [0,2,3]\n",
        "#                                [4,5,6]]\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "num_words = min(MAX_NB_WORDS, len(word_index)) + 1\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojPL5aPHsIo3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "aa2a8cdb-040e-403b-8f24-e40402bd2d17"
      },
      "source": [
        "embedding_matrix = create_embedding_matrix(word_index, num_words)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading glove vectors...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-368b3810e888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-2511248c199a>\u001b[0m in \u001b[0;36mcreate_embedding_matrix\u001b[0;34m(word_index, num_words)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mglove_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_glove_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2511248c199a>\u001b[0m in \u001b[0;36mload_glove_vectors\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading glove vectors...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mglove_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.%dd.txt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.50d.txt'"
          ]
        }
      ]
    }
  ]
}